{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import MovieLens\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        # z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
    "        z = torch.cat([z_dict['u'][row], z_dict['v'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'./raw/ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['if'] = df['rating'].apply(lambda x: 1 if x>3.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    52256\n",
       "1    48580\n",
       "Name: if, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['if'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node(col):\n",
    "    # x = torch.rand(len(col.unique()), 100, device=device)\n",
    "    mapping = {id_: i for i, id_ in enumerate(col.unique())}\n",
    "    x = torch.eye(len(col.unique()))\n",
    "    return x, mapping\n",
    "\n",
    "def load_edge(src, dst, u_mapping, v_mapping):\n",
    "    src_mapping = [u_mapping[u] for u in src.tolist()]\n",
    "    dst_mapping = [v_mapping[v] for v in dst.tolist()]\n",
    "\n",
    "    return torch.tensor([src_mapping, dst_mapping], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mu\u001b[0m={ x=[610, 610] },\n",
       "  \u001b[1mv\u001b[0m={ x=[9724, 9724] },\n",
       "  \u001b[1m(u, r, v)\u001b[0m={\n",
       "    edge_index=[2, 100836],\n",
       "    edge_label=[100836]\n",
       "  },\n",
       "  \u001b[1m(v, rev_r, u)\u001b[0m={ edge_index=[2, 100836] }\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = HeteroData()\n",
    "ux, u_mapping = load_node(df['userId'])\n",
    "vx, v_mapping = load_node(df['movieId'])\n",
    "\n",
    "data['u'].x = ux\n",
    "data['v'].x = vx\n",
    "data['u', 'r', 'v'].edge_index = load_edge(df['userId'], df['movieId'], u_mapping, v_mapping)\n",
    "data['u', 'r', 'v'].edge_label = torch.tensor(df['if'].tolist(), device=device)\n",
    "\n",
    "data = T.ToUndirected()(data)\n",
    "del data['v', 'rev_r', 'u'].edge_label\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HeteroData(\n",
    "#   movie={ x=[9742, 404] },\n",
    "#   user={ x=[610, 610] },\n",
    "#   (user, rates, movie)={\n",
    "#     edge_index=[2, 100836],\n",
    "#     edge_label=[100836]\n",
    "#   },\n",
    "#   (movie, rev_rates, user)={ edge_index=[2, 100836] }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = T.AddSelfLoops()(data)\n",
    "data = T.NormalizeFeatures()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device=device)\n",
    "\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('u', 'r', 'v')],\n",
    "    rev_edge_types=[('v', 'rev_r', 'u')],\n",
    ")(data)\n",
    "\n",
    "# use_weighted_loss = True\n",
    "# # We have an unbalanced dataset with many labels for rating 3 and 4, and very\n",
    "# # few for 0 and 1. Therefore we use a weighted MSE loss.\n",
    "# if use_weighted_loss:\n",
    "#     weight = torch.bincount(train_data['u', 'v'].edge_label)\n",
    "#     weight = weight.max() / weight\n",
    "# else:\n",
    "#     weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['u', 'v'].edge_label_index)\n",
    "    target = train_data['u', 'v'].edge_label\n",
    "    loss = criterion(pred, target.float())\n",
    "    # loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['u', 'v'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    target = data['u', 'v'].edge_label\n",
    "    log_loss_ = log_loss(target.cpu().numpy().astype(int), pred.cpu().numpy())\n",
    "    \n",
    "    return float(log_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sag = SAGEConv((-1, -1), 32)\n",
    "# sag_ = to_hetero(sag, data.metadata(), aggr='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sag(train_data.x_dict['u'], train_data.edge_index_dict[('u', 'r', 'v')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_channels=32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Due to lazy initialization, we need to run one model step so the number\n",
    "# of parameters can be inferred:\n",
    "with torch.no_grad():\n",
    "    model.encoder(train_data.x_dict, train_data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6938, Train: 0.6931, Val: 0.6931, Test: 0.6931\n",
      "Epoch: 002, Loss: 0.6916, Train: 0.6931, Val: 0.6931, Test: 0.6931\n",
      "Epoch: 003, Loss: 0.6899, Train: 0.6931, Val: 0.6931, Test: 0.6931\n",
      "Epoch: 004, Loss: 0.6855, Train: 0.6931, Val: 0.6931, Test: 0.6931\n",
      "Epoch: 005, Loss: 0.6774, Train: 0.6905, Val: 0.6911, Test: 0.6911\n",
      "Epoch: 006, Loss: 0.6636, Train: 0.6826, Val: 0.6843, Test: 0.6844\n",
      "Epoch: 007, Loss: 0.6446, Train: 0.6757, Val: 0.6783, Test: 0.6784\n",
      "Epoch: 008, Loss: 0.6231, Train: 0.6666, Val: 0.6708, Test: 0.6711\n",
      "Epoch: 009, Loss: 0.5993, Train: 0.6531, Val: 0.6603, Test: 0.6602\n",
      "Epoch: 010, Loss: 0.5744, Train: 0.6411, Val: 0.6533, Test: 0.6522\n",
      "Epoch: 011, Loss: 0.5533, Train: 0.6379, Val: 0.6536, Test: 0.6522\n",
      "Epoch: 012, Loss: 0.5387, Train: 0.6356, Val: 0.6547, Test: 0.6526\n",
      "Epoch: 013, Loss: 0.5308, Train: 0.6314, Val: 0.6524, Test: 0.6505\n",
      "Epoch: 014, Loss: 0.5226, Train: 0.6281, Val: 0.6518, Test: 0.6497\n",
      "Epoch: 015, Loss: 0.5122, Train: 0.6248, Val: 0.6495, Test: 0.6478\n",
      "Epoch: 016, Loss: 0.5031, Train: 0.6266, Val: 0.6547, Test: 0.6528\n",
      "Epoch: 017, Loss: 0.4993, Train: 0.6236, Val: 0.6493, Test: 0.6483\n",
      "Epoch: 018, Loss: 0.4945, Train: 0.6227, Val: 0.6517, Test: 0.6500\n",
      "Epoch: 019, Loss: 0.4867, Train: 0.6239, Val: 0.6551, Test: 0.6533\n",
      "Epoch: 020, Loss: 0.4884, Train: 0.6224, Val: 0.6524, Test: 0.6504\n",
      "Epoch: 021, Loss: 0.4841, Train: 0.6231, Val: 0.6518, Test: 0.6494\n",
      "Epoch: 022, Loss: 0.4852, Train: 0.6217, Val: 0.6528, Test: 0.6505\n",
      "Epoch: 023, Loss: 0.4811, Train: 0.6217, Val: 0.6533, Test: 0.6513\n",
      "Epoch: 024, Loss: 0.4807, Train: 0.6208, Val: 0.6507, Test: 0.6482\n",
      "Epoch: 025, Loss: 0.4767, Train: 0.6209, Val: 0.6494, Test: 0.6469\n",
      "Epoch: 026, Loss: 0.4768, Train: 0.6203, Val: 0.6500, Test: 0.6478\n",
      "Epoch: 027, Loss: 0.4742, Train: 0.6208, Val: 0.6505, Test: 0.6485\n",
      "Epoch: 028, Loss: 0.4749, Train: 0.6197, Val: 0.6485, Test: 0.6463\n",
      "Epoch: 029, Loss: 0.4729, Train: 0.6196, Val: 0.6477, Test: 0.6455\n",
      "Epoch: 030, Loss: 0.4732, Train: 0.6195, Val: 0.6485, Test: 0.6461\n",
      "Epoch: 031, Loss: 0.4712, Train: 0.6197, Val: 0.6491, Test: 0.6466\n",
      "Epoch: 032, Loss: 0.4712, Train: 0.6191, Val: 0.6477, Test: 0.6454\n",
      "Epoch: 033, Loss: 0.4696, Train: 0.6191, Val: 0.6472, Test: 0.6453\n",
      "Epoch: 034, Loss: 0.4699, Train: 0.6189, Val: 0.6480, Test: 0.6457\n",
      "Epoch: 035, Loss: 0.4687, Train: 0.6190, Val: 0.6488, Test: 0.6464\n",
      "Epoch: 036, Loss: 0.4691, Train: 0.6187, Val: 0.6481, Test: 0.6460\n",
      "Epoch: 037, Loss: 0.4681, Train: 0.6188, Val: 0.6477, Test: 0.6458\n",
      "Epoch: 038, Loss: 0.4681, Train: 0.6185, Val: 0.6483, Test: 0.6462\n",
      "Epoch: 039, Loss: 0.4671, Train: 0.6185, Val: 0.6494, Test: 0.6471\n",
      "Epoch: 040, Loss: 0.4671, Train: 0.6182, Val: 0.6487, Test: 0.6465\n",
      "Epoch: 041, Loss: 0.4662, Train: 0.6182, Val: 0.6481, Test: 0.6461\n",
      "Epoch: 042, Loss: 0.4663, Train: 0.6180, Val: 0.6486, Test: 0.6467\n",
      "Epoch: 043, Loss: 0.4657, Train: 0.6181, Val: 0.6491, Test: 0.6472\n",
      "Epoch: 044, Loss: 0.4658, Train: 0.6178, Val: 0.6482, Test: 0.6465\n",
      "Epoch: 045, Loss: 0.4652, Train: 0.6178, Val: 0.6479, Test: 0.6462\n",
      "Epoch: 046, Loss: 0.4651, Train: 0.6178, Val: 0.6484, Test: 0.6467\n",
      "Epoch: 047, Loss: 0.4647, Train: 0.6178, Val: 0.6485, Test: 0.6468\n",
      "Epoch: 048, Loss: 0.4645, Train: 0.6177, Val: 0.6478, Test: 0.6462\n",
      "Epoch: 049, Loss: 0.4643, Train: 0.6177, Val: 0.6478, Test: 0.6462\n",
      "Epoch: 050, Loss: 0.4641, Train: 0.6177, Val: 0.6485, Test: 0.6466\n",
      "Epoch: 051, Loss: 0.4640, Train: 0.6177, Val: 0.6481, Test: 0.6464\n",
      "Epoch: 052, Loss: 0.4636, Train: 0.6178, Val: 0.6479, Test: 0.6463\n",
      "Epoch: 053, Loss: 0.4635, Train: 0.6177, Val: 0.6483, Test: 0.6464\n",
      "Epoch: 054, Loss: 0.4632, Train: 0.6177, Val: 0.6485, Test: 0.6465\n",
      "Epoch: 055, Loss: 0.4631, Train: 0.6177, Val: 0.6482, Test: 0.6462\n",
      "Epoch: 056, Loss: 0.4629, Train: 0.6177, Val: 0.6484, Test: 0.6462\n",
      "Epoch: 057, Loss: 0.4627, Train: 0.6177, Val: 0.6488, Test: 0.6464\n",
      "Epoch: 058, Loss: 0.4626, Train: 0.6176, Val: 0.6485, Test: 0.6461\n",
      "Epoch: 059, Loss: 0.4623, Train: 0.6176, Val: 0.6484, Test: 0.6460\n",
      "Epoch: 060, Loss: 0.4621, Train: 0.6176, Val: 0.6489, Test: 0.6462\n",
      "Epoch: 061, Loss: 0.4620, Train: 0.6176, Val: 0.6485, Test: 0.6459\n",
      "Epoch: 062, Loss: 0.4618, Train: 0.6175, Val: 0.6485, Test: 0.6459\n",
      "Epoch: 063, Loss: 0.4616, Train: 0.6176, Val: 0.6487, Test: 0.6460\n",
      "Epoch: 064, Loss: 0.4614, Train: 0.6175, Val: 0.6483, Test: 0.6457\n",
      "Epoch: 065, Loss: 0.4612, Train: 0.6175, Val: 0.6485, Test: 0.6459\n",
      "Epoch: 066, Loss: 0.4609, Train: 0.6175, Val: 0.6485, Test: 0.6460\n",
      "Epoch: 067, Loss: 0.4607, Train: 0.6175, Val: 0.6481, Test: 0.6458\n",
      "Epoch: 068, Loss: 0.4605, Train: 0.6176, Val: 0.6484, Test: 0.6462\n",
      "Epoch: 069, Loss: 0.4602, Train: 0.6176, Val: 0.6478, Test: 0.6458\n",
      "Epoch: 070, Loss: 0.4600, Train: 0.6177, Val: 0.6484, Test: 0.6464\n",
      "Epoch: 071, Loss: 0.4597, Train: 0.6175, Val: 0.6478, Test: 0.6459\n",
      "Epoch: 072, Loss: 0.4593, Train: 0.6176, Val: 0.6484, Test: 0.6464\n",
      "Epoch: 073, Loss: 0.4588, Train: 0.6175, Val: 0.6478, Test: 0.6460\n",
      "Epoch: 074, Loss: 0.4584, Train: 0.6176, Val: 0.6483, Test: 0.6464\n",
      "Epoch: 075, Loss: 0.4578, Train: 0.6174, Val: 0.6477, Test: 0.6459\n",
      "Epoch: 076, Loss: 0.4573, Train: 0.6175, Val: 0.6485, Test: 0.6466\n",
      "Epoch: 077, Loss: 0.4568, Train: 0.6174, Val: 0.6475, Test: 0.6456\n",
      "Epoch: 078, Loss: 0.4566, Train: 0.6179, Val: 0.6497, Test: 0.6476\n",
      "Epoch: 079, Loss: 0.4567, Train: 0.6176, Val: 0.6474, Test: 0.6455\n",
      "Epoch: 080, Loss: 0.4564, Train: 0.6173, Val: 0.6496, Test: 0.6471\n",
      "Epoch: 081, Loss: 0.4541, Train: 0.6166, Val: 0.6485, Test: 0.6458\n",
      "Epoch: 082, Loss: 0.4518, Train: 0.6164, Val: 0.6480, Test: 0.6455\n",
      "Epoch: 083, Loss: 0.4511, Train: 0.6170, Val: 0.6504, Test: 0.6475\n",
      "Epoch: 084, Loss: 0.4511, Train: 0.6162, Val: 0.6477, Test: 0.6453\n",
      "Epoch: 085, Loss: 0.4496, Train: 0.6154, Val: 0.6491, Test: 0.6461\n",
      "Epoch: 086, Loss: 0.4462, Train: 0.6149, Val: 0.6492, Test: 0.6460\n",
      "Epoch: 087, Loss: 0.4441, Train: 0.6146, Val: 0.6479, Test: 0.6453\n",
      "Epoch: 088, Loss: 0.4438, Train: 0.6148, Val: 0.6506, Test: 0.6476\n",
      "Epoch: 089, Loss: 0.4427, Train: 0.6135, Val: 0.6480, Test: 0.6462\n",
      "Epoch: 090, Loss: 0.4396, Train: 0.6125, Val: 0.6488, Test: 0.6466\n",
      "Epoch: 091, Loss: 0.4359, Train: 0.6127, Val: 0.6504, Test: 0.6476\n",
      "Epoch: 092, Loss: 0.4352, Train: 0.6120, Val: 0.6484, Test: 0.6479\n",
      "Epoch: 093, Loss: 0.4356, Train: 0.6115, Val: 0.6511, Test: 0.6488\n",
      "Epoch: 094, Loss: 0.4334, Train: 0.6102, Val: 0.6495, Test: 0.6481\n",
      "Epoch: 095, Loss: 0.4289, Train: 0.6100, Val: 0.6497, Test: 0.6485\n",
      "Epoch: 096, Loss: 0.4288, Train: 0.6102, Val: 0.6530, Test: 0.6503\n",
      "Epoch: 097, Loss: 0.4297, Train: 0.6093, Val: 0.6507, Test: 0.6495\n",
      "Epoch: 098, Loss: 0.4266, Train: 0.6081, Val: 0.6517, Test: 0.6496\n",
      "Epoch: 099, Loss: 0.4222, Train: 0.6090, Val: 0.6542, Test: 0.6510\n",
      "Epoch: 100, Loss: 0.4240, Train: 0.6090, Val: 0.6520, Test: 0.6505\n",
      "Epoch: 101, Loss: 0.4248, Train: 0.6070, Val: 0.6528, Test: 0.6511\n",
      "Epoch: 102, Loss: 0.4207, Train: 0.6076, Val: 0.6540, Test: 0.6509\n",
      "Epoch: 103, Loss: 0.4191, Train: 0.6074, Val: 0.6522, Test: 0.6498\n",
      "Epoch: 104, Loss: 0.4202, Train: 0.6063, Val: 0.6533, Test: 0.6515\n",
      "Epoch: 105, Loss: 0.4177, Train: 0.6054, Val: 0.6526, Test: 0.6510\n",
      "Epoch: 106, Loss: 0.4136, Train: 0.6061, Val: 0.6528, Test: 0.6505\n",
      "Epoch: 107, Loss: 0.4153, Train: 0.6056, Val: 0.6547, Test: 0.6518\n",
      "Epoch: 108, Loss: 0.4130, Train: 0.6049, Val: 0.6536, Test: 0.6518\n",
      "Epoch: 109, Loss: 0.4105, Train: 0.6052, Val: 0.6535, Test: 0.6516\n",
      "Epoch: 110, Loss: 0.4104, Train: 0.6042, Val: 0.6550, Test: 0.6519\n",
      "Epoch: 111, Loss: 0.4085, Train: 0.6037, Val: 0.6541, Test: 0.6514\n",
      "Epoch: 112, Loss: 0.4061, Train: 0.6037, Val: 0.6540, Test: 0.6516\n",
      "Epoch: 113, Loss: 0.4052, Train: 0.6032, Val: 0.6546, Test: 0.6520\n",
      "Epoch: 114, Loss: 0.4046, Train: 0.6033, Val: 0.6541, Test: 0.6518\n",
      "Epoch: 115, Loss: 0.4034, Train: 0.6028, Val: 0.6551, Test: 0.6523\n",
      "Epoch: 116, Loss: 0.4018, Train: 0.6030, Val: 0.6549, Test: 0.6526\n",
      "Epoch: 117, Loss: 0.4010, Train: 0.6024, Val: 0.6552, Test: 0.6528\n",
      "Epoch: 118, Loss: 0.3992, Train: 0.6025, Val: 0.6554, Test: 0.6528\n",
      "Epoch: 119, Loss: 0.3987, Train: 0.6018, Val: 0.6552, Test: 0.6529\n",
      "Epoch: 120, Loss: 0.3966, Train: 0.6023, Val: 0.6548, Test: 0.6529\n",
      "Epoch: 121, Loss: 0.3965, Train: 0.6016, Val: 0.6549, Test: 0.6529\n",
      "Epoch: 122, Loss: 0.3952, Train: 0.6019, Val: 0.6543, Test: 0.6523\n",
      "Epoch: 123, Loss: 0.3961, Train: 0.6016, Val: 0.6556, Test: 0.6540\n",
      "Epoch: 124, Loss: 0.3975, Train: 0.6030, Val: 0.6546, Test: 0.6528\n",
      "Epoch: 125, Loss: 0.3952, Train: 0.6008, Val: 0.6542, Test: 0.6530\n",
      "Epoch: 126, Loss: 0.3908, Train: 0.6009, Val: 0.6555, Test: 0.6540\n",
      "Epoch: 127, Loss: 0.3917, Train: 0.6025, Val: 0.6546, Test: 0.6528\n",
      "Epoch: 128, Loss: 0.3942, Train: 0.6008, Val: 0.6557, Test: 0.6544\n",
      "Epoch: 129, Loss: 0.3957, Train: 0.5999, Val: 0.6549, Test: 0.6532\n",
      "Epoch: 130, Loss: 0.3869, Train: 0.6019, Val: 0.6539, Test: 0.6519\n",
      "Epoch: 131, Loss: 0.4000, Train: 0.6019, Val: 0.6569, Test: 0.6553\n",
      "Epoch: 132, Loss: 0.3993, Train: 0.6016, Val: 0.6555, Test: 0.6543\n",
      "Epoch: 133, Loss: 0.3950, Train: 0.6024, Val: 0.6544, Test: 0.6537\n",
      "Epoch: 134, Loss: 0.3912, Train: 0.6011, Val: 0.6550, Test: 0.6540\n",
      "Epoch: 135, Loss: 0.3903, Train: 0.6009, Val: 0.6566, Test: 0.6559\n",
      "Epoch: 136, Loss: 0.3893, Train: 0.6008, Val: 0.6550, Test: 0.6549\n",
      "Epoch: 137, Loss: 0.3859, Train: 0.6019, Val: 0.6553, Test: 0.6556\n",
      "Epoch: 138, Loss: 0.3872, Train: 0.5985, Val: 0.6550, Test: 0.6549\n",
      "Epoch: 139, Loss: 0.3813, Train: 0.6000, Val: 0.6554, Test: 0.6548\n",
      "Epoch: 140, Loss: 0.3843, Train: 0.5986, Val: 0.6539, Test: 0.6532\n",
      "Epoch: 141, Loss: 0.3793, Train: 0.5989, Val: 0.6543, Test: 0.6544\n",
      "Epoch: 142, Loss: 0.3806, Train: 0.5984, Val: 0.6541, Test: 0.6551\n",
      "Epoch: 143, Loss: 0.3787, Train: 0.5979, Val: 0.6537, Test: 0.6544\n",
      "Epoch: 144, Loss: 0.3775, Train: 0.5983, Val: 0.6560, Test: 0.6559\n",
      "Epoch: 145, Loss: 0.3780, Train: 0.5968, Val: 0.6543, Test: 0.6547\n",
      "Epoch: 146, Loss: 0.3730, Train: 0.5986, Val: 0.6545, Test: 0.6551\n",
      "Epoch: 147, Loss: 0.3756, Train: 0.5968, Val: 0.6547, Test: 0.6556\n",
      "Epoch: 148, Loss: 0.3728, Train: 0.5964, Val: 0.6551, Test: 0.6557\n",
      "Epoch: 149, Loss: 0.3714, Train: 0.5967, Val: 0.6547, Test: 0.6555\n",
      "Epoch: 150, Loss: 0.3721, Train: 0.5956, Val: 0.6550, Test: 0.6566\n",
      "Epoch: 151, Loss: 0.3700, Train: 0.5964, Val: 0.6543, Test: 0.6562\n",
      "Epoch: 152, Loss: 0.3693, Train: 0.5959, Val: 0.6541, Test: 0.6557\n",
      "Epoch: 153, Loss: 0.3676, Train: 0.5956, Val: 0.6563, Test: 0.6571\n",
      "Epoch: 154, Loss: 0.3693, Train: 0.5952, Val: 0.6549, Test: 0.6566\n",
      "Epoch: 155, Loss: 0.3662, Train: 0.5950, Val: 0.6554, Test: 0.6573\n",
      "Epoch: 156, Loss: 0.3652, Train: 0.5946, Val: 0.6554, Test: 0.6572\n",
      "Epoch: 157, Loss: 0.3643, Train: 0.5947, Val: 0.6551, Test: 0.6566\n",
      "Epoch: 158, Loss: 0.3639, Train: 0.5942, Val: 0.6562, Test: 0.6577\n",
      "Epoch: 159, Loss: 0.3632, Train: 0.5942, Val: 0.6556, Test: 0.6578\n",
      "Epoch: 160, Loss: 0.3618, Train: 0.5938, Val: 0.6560, Test: 0.6583\n",
      "Epoch: 161, Loss: 0.3611, Train: 0.5933, Val: 0.6561, Test: 0.6582\n",
      "Epoch: 162, Loss: 0.3599, Train: 0.5935, Val: 0.6559, Test: 0.6580\n",
      "Epoch: 163, Loss: 0.3598, Train: 0.5929, Val: 0.6564, Test: 0.6587\n",
      "Epoch: 164, Loss: 0.3592, Train: 0.5938, Val: 0.6556, Test: 0.6586\n",
      "Epoch: 165, Loss: 0.3597, Train: 0.5930, Val: 0.6579, Test: 0.6598\n",
      "Epoch: 166, Loss: 0.3609, Train: 0.5932, Val: 0.6555, Test: 0.6581\n",
      "Epoch: 167, Loss: 0.3583, Train: 0.5924, Val: 0.6564, Test: 0.6591\n",
      "Epoch: 168, Loss: 0.3564, Train: 0.5922, Val: 0.6571, Test: 0.6597\n",
      "Epoch: 169, Loss: 0.3561, Train: 0.5928, Val: 0.6561, Test: 0.6590\n",
      "Epoch: 170, Loss: 0.3565, Train: 0.5921, Val: 0.6577, Test: 0.6600\n",
      "Epoch: 171, Loss: 0.3566, Train: 0.5927, Val: 0.6559, Test: 0.6591\n",
      "Epoch: 172, Loss: 0.3553, Train: 0.5917, Val: 0.6568, Test: 0.6598\n",
      "Epoch: 173, Loss: 0.3535, Train: 0.5914, Val: 0.6573, Test: 0.6598\n",
      "Epoch: 174, Loss: 0.3531, Train: 0.5919, Val: 0.6564, Test: 0.6592\n",
      "Epoch: 175, Loss: 0.3533, Train: 0.5918, Val: 0.6583, Test: 0.6609\n",
      "Epoch: 176, Loss: 0.3558, Train: 0.5921, Val: 0.6565, Test: 0.6590\n",
      "Epoch: 177, Loss: 0.3531, Train: 0.5910, Val: 0.6581, Test: 0.6607\n",
      "Epoch: 178, Loss: 0.3517, Train: 0.5910, Val: 0.6577, Test: 0.6605\n",
      "Epoch: 179, Loss: 0.3502, Train: 0.5911, Val: 0.6572, Test: 0.6598\n",
      "Epoch: 180, Loss: 0.3498, Train: 0.5907, Val: 0.6578, Test: 0.6603\n",
      "Epoch: 181, Loss: 0.3503, Train: 0.5916, Val: 0.6571, Test: 0.6598\n",
      "Epoch: 182, Loss: 0.3514, Train: 0.5915, Val: 0.6603, Test: 0.6623\n",
      "Epoch: 183, Loss: 0.3547, Train: 0.5914, Val: 0.6560, Test: 0.6589\n",
      "Epoch: 184, Loss: 0.3501, Train: 0.5903, Val: 0.6573, Test: 0.6602\n",
      "Epoch: 185, Loss: 0.3477, Train: 0.5898, Val: 0.6589, Test: 0.6621\n",
      "Epoch: 186, Loss: 0.3479, Train: 0.5911, Val: 0.6570, Test: 0.6601\n",
      "Epoch: 187, Loss: 0.3489, Train: 0.5906, Val: 0.6590, Test: 0.6614\n",
      "Epoch: 188, Loss: 0.3508, Train: 0.5903, Val: 0.6576, Test: 0.6604\n",
      "Epoch: 189, Loss: 0.3469, Train: 0.5892, Val: 0.6591, Test: 0.6624\n",
      "Epoch: 190, Loss: 0.3452, Train: 0.5892, Val: 0.6584, Test: 0.6614\n",
      "Epoch: 191, Loss: 0.3449, Train: 0.5903, Val: 0.6571, Test: 0.6595\n",
      "Epoch: 192, Loss: 0.3461, Train: 0.5896, Val: 0.6606, Test: 0.6627\n",
      "Epoch: 193, Loss: 0.3478, Train: 0.5899, Val: 0.6584, Test: 0.6610\n",
      "Epoch: 194, Loss: 0.3450, Train: 0.5888, Val: 0.6587, Test: 0.6612\n",
      "Epoch: 195, Loss: 0.3432, Train: 0.5887, Val: 0.6583, Test: 0.6608\n",
      "Epoch: 196, Loss: 0.3420, Train: 0.5887, Val: 0.6585, Test: 0.6614\n",
      "Epoch: 197, Loss: 0.3417, Train: 0.5884, Val: 0.6595, Test: 0.6618\n",
      "Epoch: 198, Loss: 0.3425, Train: 0.5898, Val: 0.6581, Test: 0.6607\n",
      "Epoch: 199, Loss: 0.3440, Train: 0.5891, Val: 0.6611, Test: 0.6631\n",
      "Epoch: 200, Loss: 0.3465, Train: 0.5890, Val: 0.6574, Test: 0.6603\n",
      "Epoch: 201, Loss: 0.3421, Train: 0.5879, Val: 0.6585, Test: 0.6616\n",
      "Epoch: 202, Loss: 0.3399, Train: 0.5875, Val: 0.6594, Test: 0.6622\n",
      "Epoch: 203, Loss: 0.3393, Train: 0.5883, Val: 0.6583, Test: 0.6612\n",
      "Epoch: 204, Loss: 0.3396, Train: 0.5878, Val: 0.6601, Test: 0.6629\n",
      "Epoch: 205, Loss: 0.3408, Train: 0.5885, Val: 0.6584, Test: 0.6616\n",
      "Epoch: 206, Loss: 0.3401, Train: 0.5869, Val: 0.6599, Test: 0.6633\n",
      "Epoch: 207, Loss: 0.3382, Train: 0.5871, Val: 0.6579, Test: 0.6612\n",
      "Epoch: 208, Loss: 0.3363, Train: 0.5870, Val: 0.6590, Test: 0.6620\n",
      "Epoch: 209, Loss: 0.3358, Train: 0.5865, Val: 0.6607, Test: 0.6640\n",
      "Epoch: 210, Loss: 0.3361, Train: 0.5873, Val: 0.6584, Test: 0.6610\n",
      "Epoch: 211, Loss: 0.3362, Train: 0.5869, Val: 0.6610, Test: 0.6631\n",
      "Epoch: 212, Loss: 0.3377, Train: 0.5878, Val: 0.6592, Test: 0.6624\n",
      "Epoch: 213, Loss: 0.3369, Train: 0.5862, Val: 0.6609, Test: 0.6634\n",
      "Epoch: 214, Loss: 0.3343, Train: 0.5860, Val: 0.6590, Test: 0.6617\n",
      "Epoch: 215, Loss: 0.3324, Train: 0.5861, Val: 0.6597, Test: 0.6627\n",
      "Epoch: 216, Loss: 0.3324, Train: 0.5859, Val: 0.6620, Test: 0.6641\n",
      "Epoch: 217, Loss: 0.3338, Train: 0.5871, Val: 0.6585, Test: 0.6613\n",
      "Epoch: 218, Loss: 0.3348, Train: 0.5863, Val: 0.6625, Test: 0.6642\n",
      "Epoch: 219, Loss: 0.3362, Train: 0.5864, Val: 0.6595, Test: 0.6622\n",
      "Epoch: 220, Loss: 0.3324, Train: 0.5861, Val: 0.6603, Test: 0.6631\n",
      "Epoch: 221, Loss: 0.3321, Train: 0.5854, Val: 0.6622, Test: 0.6636\n",
      "Epoch: 222, Loss: 0.3313, Train: 0.5859, Val: 0.6604, Test: 0.6630\n",
      "Epoch: 223, Loss: 0.3304, Train: 0.5859, Val: 0.6621, Test: 0.6643\n",
      "Epoch: 224, Loss: 0.3317, Train: 0.5855, Val: 0.6610, Test: 0.6631\n",
      "Epoch: 225, Loss: 0.3295, Train: 0.5844, Val: 0.6609, Test: 0.6630\n",
      "Epoch: 226, Loss: 0.3275, Train: 0.5852, Val: 0.6611, Test: 0.6634\n",
      "Epoch: 227, Loss: 0.3290, Train: 0.5849, Val: 0.6620, Test: 0.6636\n",
      "Epoch: 228, Loss: 0.3278, Train: 0.5841, Val: 0.6616, Test: 0.6635\n",
      "Epoch: 229, Loss: 0.3256, Train: 0.5847, Val: 0.6617, Test: 0.6635\n",
      "Epoch: 230, Loss: 0.3269, Train: 0.5847, Val: 0.6622, Test: 0.6636\n",
      "Epoch: 231, Loss: 0.3268, Train: 0.5836, Val: 0.6620, Test: 0.6636\n",
      "Epoch: 232, Loss: 0.3242, Train: 0.5844, Val: 0.6610, Test: 0.6629\n",
      "Epoch: 233, Loss: 0.3253, Train: 0.5843, Val: 0.6638, Test: 0.6645\n",
      "Epoch: 234, Loss: 0.3265, Train: 0.5836, Val: 0.6616, Test: 0.6633\n",
      "Epoch: 235, Loss: 0.3230, Train: 0.5841, Val: 0.6616, Test: 0.6633\n",
      "Epoch: 236, Loss: 0.3246, Train: 0.5846, Val: 0.6630, Test: 0.6640\n",
      "Epoch: 237, Loss: 0.3253, Train: 0.5830, Val: 0.6637, Test: 0.6645\n",
      "Epoch: 238, Loss: 0.3225, Train: 0.5843, Val: 0.6609, Test: 0.6622\n",
      "Epoch: 239, Loss: 0.3253, Train: 0.5845, Val: 0.6658, Test: 0.6658\n",
      "Epoch: 240, Loss: 0.3283, Train: 0.5841, Val: 0.6620, Test: 0.6632\n",
      "Epoch: 241, Loss: 0.3233, Train: 0.5848, Val: 0.6630, Test: 0.6645\n",
      "Epoch: 242, Loss: 0.3263, Train: 0.5834, Val: 0.6631, Test: 0.6638\n",
      "Epoch: 243, Loss: 0.3218, Train: 0.5827, Val: 0.6641, Test: 0.6646\n",
      "Epoch: 244, Loss: 0.3203, Train: 0.5838, Val: 0.6630, Test: 0.6641\n",
      "Epoch: 245, Loss: 0.3228, Train: 0.5827, Val: 0.6629, Test: 0.6637\n",
      "Epoch: 246, Loss: 0.3193, Train: 0.5824, Val: 0.6646, Test: 0.6648\n",
      "Epoch: 247, Loss: 0.3200, Train: 0.5833, Val: 0.6621, Test: 0.6634\n",
      "Epoch: 248, Loss: 0.3203, Train: 0.5822, Val: 0.6640, Test: 0.6644\n",
      "Epoch: 249, Loss: 0.3187, Train: 0.5836, Val: 0.6639, Test: 0.6642\n",
      "Epoch: 250, Loss: 0.3206, Train: 0.5828, Val: 0.6651, Test: 0.6658\n",
      "Epoch: 251, Loss: 0.3205, Train: 0.5836, Val: 0.6617, Test: 0.6623\n",
      "Epoch: 252, Loss: 0.3205, Train: 0.5832, Val: 0.6662, Test: 0.6662\n",
      "Epoch: 253, Loss: 0.3235, Train: 0.5825, Val: 0.6619, Test: 0.6626\n",
      "Epoch: 254, Loss: 0.3175, Train: 0.5822, Val: 0.6632, Test: 0.6640\n",
      "Epoch: 255, Loss: 0.3173, Train: 0.5816, Val: 0.6652, Test: 0.6651\n",
      "Epoch: 256, Loss: 0.3163, Train: 0.5820, Val: 0.6636, Test: 0.6637\n",
      "Epoch: 257, Loss: 0.3156, Train: 0.5823, Val: 0.6647, Test: 0.6652\n",
      "Epoch: 258, Loss: 0.3178, Train: 0.5822, Val: 0.6633, Test: 0.6636\n",
      "Epoch: 259, Loss: 0.3158, Train: 0.5813, Val: 0.6651, Test: 0.6649\n",
      "Epoch: 260, Loss: 0.3159, Train: 0.5819, Val: 0.6631, Test: 0.6638\n",
      "Epoch: 261, Loss: 0.3150, Train: 0.5808, Val: 0.6657, Test: 0.6653\n",
      "Epoch: 262, Loss: 0.3137, Train: 0.5816, Val: 0.6643, Test: 0.6641\n",
      "Epoch: 263, Loss: 0.3137, Train: 0.5810, Val: 0.6649, Test: 0.6649\n",
      "Epoch: 264, Loss: 0.3136, Train: 0.5812, Val: 0.6634, Test: 0.6638\n",
      "Epoch: 265, Loss: 0.3125, Train: 0.5805, Val: 0.6657, Test: 0.6654\n",
      "Epoch: 266, Loss: 0.3132, Train: 0.5812, Val: 0.6634, Test: 0.6641\n",
      "Epoch: 267, Loss: 0.3124, Train: 0.5803, Val: 0.6653, Test: 0.6652\n",
      "Epoch: 268, Loss: 0.3120, Train: 0.5813, Val: 0.6642, Test: 0.6644\n",
      "Epoch: 269, Loss: 0.3124, Train: 0.5803, Val: 0.6651, Test: 0.6653\n",
      "Epoch: 270, Loss: 0.3118, Train: 0.5809, Val: 0.6634, Test: 0.6636\n",
      "Epoch: 271, Loss: 0.3111, Train: 0.5802, Val: 0.6666, Test: 0.6664\n",
      "Epoch: 272, Loss: 0.3127, Train: 0.5808, Val: 0.6631, Test: 0.6634\n",
      "Epoch: 273, Loss: 0.3107, Train: 0.5796, Val: 0.6659, Test: 0.6658\n",
      "Epoch: 274, Loss: 0.3098, Train: 0.5804, Val: 0.6646, Test: 0.6645\n",
      "Epoch: 275, Loss: 0.3097, Train: 0.5796, Val: 0.6650, Test: 0.6645\n",
      "Epoch: 276, Loss: 0.3084, Train: 0.5794, Val: 0.6653, Test: 0.6648\n",
      "Epoch: 277, Loss: 0.3074, Train: 0.5793, Val: 0.6649, Test: 0.6646\n",
      "Epoch: 278, Loss: 0.3078, Train: 0.5796, Val: 0.6646, Test: 0.6641\n",
      "Epoch: 279, Loss: 0.3074, Train: 0.5790, Val: 0.6662, Test: 0.6657\n",
      "Epoch: 280, Loss: 0.3077, Train: 0.5804, Val: 0.6632, Test: 0.6633\n",
      "Epoch: 281, Loss: 0.3087, Train: 0.5802, Val: 0.6683, Test: 0.6671\n",
      "Epoch: 282, Loss: 0.3124, Train: 0.5832, Val: 0.6633, Test: 0.6631\n",
      "Epoch: 283, Loss: 0.3165, Train: 0.5817, Val: 0.6692, Test: 0.6676\n",
      "Epoch: 284, Loss: 0.3176, Train: 0.5806, Val: 0.6628, Test: 0.6623\n",
      "Epoch: 285, Loss: 0.3097, Train: 0.5786, Val: 0.6665, Test: 0.6653\n",
      "Epoch: 286, Loss: 0.3051, Train: 0.5789, Val: 0.6677, Test: 0.6662\n",
      "Epoch: 287, Loss: 0.3075, Train: 0.5809, Val: 0.6639, Test: 0.6628\n",
      "Epoch: 288, Loss: 0.3097, Train: 0.5789, Val: 0.6687, Test: 0.6667\n",
      "Epoch: 289, Loss: 0.3075, Train: 0.5786, Val: 0.6643, Test: 0.6629\n",
      "Epoch: 290, Loss: 0.3037, Train: 0.5786, Val: 0.6655, Test: 0.6641\n",
      "Epoch: 291, Loss: 0.3037, Train: 0.5783, Val: 0.6686, Test: 0.6667\n",
      "Epoch: 292, Loss: 0.3045, Train: 0.5789, Val: 0.6647, Test: 0.6637\n",
      "Epoch: 293, Loss: 0.3035, Train: 0.5784, Val: 0.6670, Test: 0.6652\n",
      "Epoch: 294, Loss: 0.3050, Train: 0.5784, Val: 0.6661, Test: 0.6649\n",
      "Epoch: 295, Loss: 0.3031, Train: 0.5778, Val: 0.6657, Test: 0.6645\n",
      "Epoch: 296, Loss: 0.3016, Train: 0.5778, Val: 0.6668, Test: 0.6660\n",
      "Epoch: 297, Loss: 0.3016, Train: 0.5777, Val: 0.6666, Test: 0.6655\n",
      "Epoch: 298, Loss: 0.3012, Train: 0.5772, Val: 0.6660, Test: 0.6652\n",
      "Epoch: 299, Loss: 0.3002, Train: 0.5775, Val: 0.6658, Test: 0.6649\n",
      "Epoch: 300, Loss: 0.3001, Train: 0.5772, Val: 0.6670, Test: 0.6656\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    test_rmse = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'./'\n",
    "dataset = MovieLens(path, model_name='all-MiniLM-L6-v2')\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "# Add user node features for message passing:\n",
    "data['user'].x = torch.eye(data['user'].num_nodes, device=device)\n",
    "del data['user'].num_nodes\n",
    "\n",
    "# Add a reverse ('movie', 'rev_rates', 'user') relation for message passing:\n",
    "data = T.ToUndirected()(data)\n",
    "del data['movie', 'rev_rates', 'user'].edge_label  # Remove \"reverse\" label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mmovie\u001b[0m={ x=[9742, 404] },\n",
       "  \u001b[1muser\u001b[0m={ x=[610, 610] },\n",
       "  \u001b[1m(user, rates, movie)\u001b[0m={\n",
       "    edge_index=[2, 100836],\n",
       "    edge_label=[100836]\n",
       "  },\n",
       "  \u001b[1m(movie, rev_rates, user)\u001b[0m={ edge_index=[2, 100836] }\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9741, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user', 'rates', 'movie'].edge_index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform a link-level split into training, validation, and test edges:\n",
    "train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    edge_types=[('user', 'rates', 'movie')],\n",
    "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
    ")(data)\n",
    "\n",
    "use_weighted_loss = True\n",
    "# We have an unbalanced dataset with many labels for rating 3 and 4, and very\n",
    "# few for 0 and 1. Therefore we use a weighted MSE loss.\n",
    "if use_weighted_loss:\n",
    "    weight = torch.bincount(train_data['user', 'movie'].edge_label)\n",
    "    weight = weight.max() / weight\n",
    "else:\n",
    "    weight = None\n",
    "\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['user', 'movie'].edge_label_index)\n",
    "    target = train_data['user', 'movie'].edge_label\n",
    "    loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['user', 'movie'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
    "                 train_data['u', 'v'].edge_label_index)\n",
    "    target = train_data['u', 'v'].edge_label\n",
    "    loss = criterion(pred, target.float())\n",
    "    # loss = weighted_mse_loss(pred, target, weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                 data['user', 'movie'].edge_label_index)\n",
    "    pred = pred.clamp(min=0, max=1)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    target = data['user', 'movie'].edge_label\n",
    "    log_loss_ = log_loss(target.cpu().numpy().astype(int), pred.cpu().numpy())\n",
    "    \n",
    "    return float(log_loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_channels=32).to(device)\n",
    "\n",
    "# Due to lazy initialization, we need to run one model step so the number\n",
    "# of parameters can be inferred:\n",
    "with torch.no_grad():\n",
    "    model.encoder(train_data.x_dict, train_data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EdgeStorage' object has no attribute 'edge_label_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32me:\\Software\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\storage.py:50\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[0;32m     51\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Software\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\storage.py:70\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'edge_label_index'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Workplace\\bip_likn_predict.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m301\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_rmse \u001b[39m=\u001b[39m test(train_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     val_rmse \u001b[39m=\u001b[39m test(val_data)\n",
      "\u001b[1;32me:\\Workplace\\bip_likn_predict.ipynb Cell 27\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pred \u001b[39m=\u001b[39m model(train_data\u001b[39m.\u001b[39mx_dict, train_data\u001b[39m.\u001b[39medge_index_dict,\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m              train_data[\u001b[39m'\u001b[39;49m\u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mv\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49medge_label_index)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m target \u001b[39m=\u001b[39m train_data[\u001b[39m'\u001b[39m\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mv\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39medge_label\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Workplace/bip_likn_predict.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, target\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[1;32me:\\Software\\anaconda3\\lib\\site-packages\\torch_geometric\\data\\storage.py:52\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[0;32m     51\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EdgeStorage' object has no attribute 'edge_label_index'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    test_rmse = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2b388c6fce79e00fd9c43dd7c300c62775de93114fdc7222b9aeb8ab89a5a93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

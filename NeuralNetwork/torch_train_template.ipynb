{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(str(info)+\"\\n\")\n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn,\n",
    "                 stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer = optimizer\n",
    "            \n",
    "    def step(self, features, labels):\n",
    "        #loss\n",
    "        preds = self.net(features)\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "        \n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\": \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        #metrics\n",
    "        step_metrics = {self.stage+\"_\"+name:metric_fn(preds, labels).item() \n",
    "                        for name,metric_fn in self.metrics_dict.items()}\n",
    "        return loss.item(),step_metrics\n",
    "    \n",
    "    def train_step(self,features,labels):\n",
    "        self.net.train() #训练模式, dropout层发生作用\n",
    "        return self.step(features,labels)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def eval_step(self,features,labels):\n",
    "        self.net.eval() #预测模式, dropout层不发生作用\n",
    "        return self.step(features,labels)\n",
    "    \n",
    "    def __call__(self,features,labels):\n",
    "        if self.stage==\"train\":\n",
    "            return self.train_step(features,labels) \n",
    "        else:\n",
    "            return self.eval_step(features,labels)\n",
    "        \n",
    "class EpochRunner:\n",
    "    def __init__(self,steprunner):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        \n",
    "    def __call__(self,dataloader):\n",
    "        total_loss,step = 0,0\n",
    "        loop = tqdm(enumerate(dataloader), total =len(dataloader), file = sys.stdout)\n",
    "        for i, batch in loop: \n",
    "            loss, step_metrics = self.steprunner(*batch)\n",
    "            step_log = dict({self.stage+\"_loss\":loss},**step_metrics)\n",
    "            total_loss += loss\n",
    "            step+=1\n",
    "            if i!=len(dataloader)-1:\n",
    "                loop.set_postfix(**step_log)\n",
    "            else:\n",
    "                epoch_loss = total_loss/step\n",
    "                epoch_metrics = {self.stage+\"_\"+name:metric_fn.compute().item() \n",
    "                                 for name,metric_fn in self.steprunner.metrics_dict.items()}\n",
    "                epoch_log = dict({self.stage+\"_loss\":epoch_loss},**epoch_metrics)\n",
    "                loop.set_postfix(**epoch_log)\n",
    "\n",
    "                for name,metric_fn in self.steprunner.metrics_dict.items():\n",
    "                    metric_fn.reset()\n",
    "        return epoch_log\n",
    "\n",
    "\n",
    "def train_model(net, optimizer, loss_fn, metrics_dict, \n",
    "                train_data, val_data=None, \n",
    "                epochs=10, ckpt_path='checkpoint.pt',\n",
    "                patience=5, monitor=\"val_loss\", mode=\"min\"):\n",
    "    \n",
    "    history = {}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        printlog(\"Epoch {0} / {1}\".format(epoch, epochs))\n",
    "\n",
    "        # 1，train -------------------------------------------------  \n",
    "        train_step_runner = StepRunner(net = net,stage=\"train\",\n",
    "                loss_fn = loss_fn,metrics_dict=deepcopy(metrics_dict),\n",
    "                optimizer = optimizer)\n",
    "        train_epoch_runner = EpochRunner(train_step_runner)\n",
    "        train_metrics = train_epoch_runner(train_data)\n",
    "\n",
    "        for name, metric in train_metrics.items():\n",
    "            history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "        # 2，validate -------------------------------------------------\n",
    "        if val_data:\n",
    "            val_step_runner = StepRunner(net = net,stage=\"val\",\n",
    "                loss_fn = loss_fn,metrics_dict=deepcopy(metrics_dict))\n",
    "            val_epoch_runner = EpochRunner(val_step_runner)\n",
    "            with torch.no_grad():\n",
    "                val_metrics = val_epoch_runner(val_data)\n",
    "            val_metrics[\"epoch\"] = epoch\n",
    "            for name, metric in val_metrics.items():\n",
    "                history[name] = history.get(name, []) + [metric]\n",
    "\n",
    "        # 3，early-stopping -------------------------------------------------\n",
    "        arr_scores = history[monitor]\n",
    "        best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n",
    "        if best_score_idx==len(arr_scores)-1:\n",
    "            torch.save(net.state_dict(),ckpt_path)\n",
    "            print(\"<<<<<< reach best {0} : {1} >>>>>>\".format(monitor,\n",
    "                 arr_scores[best_score_idx]))\n",
    "        if len(arr_scores)-best_score_idx>patience:\n",
    "            print(\"<<<<<< {} without improvement in {} epoch, early stopping >>>>>>\".format(\n",
    "                monitor,patience))\n",
    "            break \n",
    "        net.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer= torch.optim.Adam(net.parameters(),lr = 0.01)   \n",
    "metrics_dict = {\"acc\":Accuracy()}\n",
    "\n",
    "dfhistory = train_model(net,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    metrics_dict,\n",
    "    train_data = dl_train,\n",
    "    val_data= dl_val,\n",
    "    epochs=10,\n",
    "    patience=5,\n",
    "    monitor=\"val_acc\", \n",
    "    mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    tb.add_scalar(\"Loss\", i, i)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d5334fe9b82e8c016b8b2657765205a9a9ba0b9bc469dbf5ca7c631ff3e3ab7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

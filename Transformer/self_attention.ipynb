{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "1. [load package](#1-load-package)\n",
    "2. [首先理解：$Softmax(XX^T)X$](#2--softmaxxxtx)\n",
    "3. [$QKV$矩阵](#3-qkv矩阵)\n",
    "4. [集成self-attention类](#4-集成self-attention类)\n",
    "5. [多头数量的确定](#5-headnums的确定)\n",
    "   \n",
    "# Reference: \n",
    "* [超详细图解Self-Attention的那些事儿](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247579430&idx=1&sn=3c63a42410e2107f1f91f861dc25c3cb&chksm=ec1d5adfdb6ad3c953722280aae160d9bf8a73d4cb4374de196d2a2b5bb2b53ac6b7eecbc568&mpshare=1&scene=1&srcid=05019UXsKNB17Qv7uzylnUDI&sharer_sharetime=1651341683033&sharer_shareid=d50902e5393c9beaed62a664f9b58b17#rd)\n",
    "* [Attention Is All You Need(注意力模型）](https://zhuanlan.zhihu.com/p/44731789)\n",
    "* [Attention 和self-attention](https://zhuanlan.zhihu.com/p/109496099)\n",
    "* [超细节的BERT/Transformer知识点](https://zhuanlan.zhihu.com/p/132554155)\n",
    "\n",
    "![](https://img2020.cnblogs.com/blog/1470684/202103/1470684-20210309221818623-1103910065.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', font='SimHei')\n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda is available')\n",
    "    cuda0 = torch.device('cuda:0')\n",
    "    X = torch.tensor([\n",
    "        [1,2,1,2,1],\n",
    "        [1,1,3,2,1],\n",
    "        [3,1,2,1,1]\n",
    "    ], dtype=torch.float64, device=cuda0)# 字向量定义\n",
    "else:\n",
    "    X = torch.tensor([\n",
    "        [1,2,1,2,1],\n",
    "        [1,1,3,2,1],\n",
    "        [3,1,2,1,1]\n",
    "    ], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在有组字向量\n",
    "* 早[1,2,1,2,1]\n",
    "* 上[1,1,3,2,1]\n",
    "* 好[3,1,2,1,1]\n",
    "* 它们构成了一个句子[早上好]\n",
    "* 那么这个句子的向量则为一个矩阵，命名为$X$:\n",
    "  ```[\n",
    "        [1,2,1,2,1],\n",
    "        [1,1,3,2,1],\n",
    "        [3,1,2,1,1]\n",
    "    ]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 1., 2., 1.],\n",
       "        [1., 1., 3., 2., 1.],\n",
       "        [3., 1., 2., 1., 1.]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. $ Softmax(XX^T)X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 矩阵内积$ (XX^T) $的意义\n",
    "* 求矩阵内每个字向量之间的相互投影，即相关程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 11., 10.],\n",
       "        [11., 16., 13.],\n",
       "        [10., 13., 16.]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot = X@X.T\n",
    "X_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_corr = pd.DataFrame(X_dot.cpu().T, columns=['早', '上', '好'], index=['早', '上', '好'])\n",
    "ax = sns.heatmap(df_corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 $ Softmax() $的意义\n",
    "![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfq8debSx7RJV8NahibXzTMAYfoTEVeRf6or6JibQ9KuG1kH4DblLQlxrsgKecEoxpkSUgj1d6kjpDBQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)\n",
    "* 将进行归一化处理，转换成概率形式，使每一个字向量在其他字向量的投影的和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4223, 0.4223, 0.1554],\n",
       "        [0.0064, 0.9465, 0.0471],\n",
       "        [0.0024, 0.0473, 0.9503]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.matmul(X, X.T)\n",
    "# torch.mm(X, X.T)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "sft_X_dot = softmax(X @ X.T)\n",
    "sft_X_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_corr_weight = pd.DataFrame(sft_X_dot.cpu(), columns=['早', '上', '好'], index=['早', '上', '好'])\n",
    "ax = sns.heatmap(df_corr_weight, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 最后一步$ Softmax(XX^T)X $的意义\n",
    "![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfq8debSx7RJV8NahibXzTMAYNhw7lCUTCEpoUKNH2CEsGGqCPgNBQ1rkeYVI6cwzF4DYeX9ULXQMXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)\n",
    "* 将“早”在每个字上的加权相关（即注意力）与原矩阵$X$的一个列向量相乘，求出一个新的行向量\n",
    "* 在新的行向量中，每一个维度的数值都是由三个词向量在这一维度的数值加权求和得来的，这个新的行向量就是“早”字词向量经过注意力机制加权求和之后的表示\n",
    "* $Softmax(XX^T)$为attetnion score，但是已经很难表示原本的句子，因此还需要再重新乘于$X$，得到加权后的结果。\n",
    "* 单独一个$X$矩阵只能包含了word embedding内每个字的独立信息，无法包括字与字之间的信息，相互之间没什么关系。但是经过与attention score相乘后，$V$中每个token的向量（即一个单词的word embedding向量），在$N$(例如$N=5$在本例子)维的每个维度上（每一列）上，都会对其他token做出调整（关注度不同）。与$V$相乘这一步，相当于提纯，让每个单词关注该关注的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3107, 1.4223, 2.0000, 1.8446, 1.0000],\n",
       "        [1.0942, 1.0064, 2.9401, 1.9529, 1.0000],\n",
       "        [2.9007, 1.0024, 2.0450, 1.0497, 1.0000]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_sft_X_dot = sft_X_dot@X\n",
    "weight_sft_X_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中右半部分的颜色深浅，其实就是我们上图中黄色向量中数值的大小，意义就是单词之间的相关度（回想之前的内容，相关度其本质是由向量的内积度量的）！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfq8debSx7RJV8NahibXzTMAYawvlPOUnPibj0icbBKccm4A6iaC9QiaAbWtuXGhk3XVAz4AHttjhzUq4FA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. $QKV$矩阵\n",
    "* [Transformer中K 、Q、V的设置以及为什么不能使用同一个值](https://www.cnblogs.com/jins-note/p/14508523.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfq8debSx7RJV8NahibXzTMAYZrGVNq0AaDiamGVwhTsENJgxJCzgpAkgQhnQibDvFOHWVnFrAUNgqiccg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实，许多文章中所谓的$QKV$矩阵、查询向量之类的字眼，根本其来源都是是$X$与矩阵的乘积，**本质上都是$X$的线性变换。**\n",
    "* 为什么不直接使用$X$而要对其进行线性变换？\n",
    "* 当然是为了提升模型的拟合能力，矩阵$W$都是可以训练的，起到一个缓冲的效果。\n",
    "* 如果你真正读懂了前文的内容，读懂了$Softmax(XX^T)$这个矩阵的意义，相信你也理解了所谓查询向量一类字眼的含义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 $\\sqrt d_k$的意义\n",
    "* $d_k$为$K$的dimension\n",
    "* 假设$Softmax(QK^T)$的均值为0，方差为1，那么$d$为$Softmax(QK^T)$的方差，\n",
    "  * $d$的值越大会导致$Softmax(QK^T)$的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)\n",
    "  * 控制$Softmax(QK^T)$的分布：\n",
    "    * $Softmax(QK^T)$的分布会和$d$有关。$Softmax(QK^T)$中每一个元素除以$d$后，方差又变为1。\n",
    "    * 利用$\\sqrt d_k$对$Softmax(QK^T)$的分布“陡峭”程度与$d$解耦，从而使得训练过程中梯度值保持稳定\n",
    "* 利用$Q$输入与key-value memories做检索，找到和问题相似的memory的key计算相关性分数，然后对value embedding进行加权求和，得到一个输出向量。衍生出了self-attention里的$Q$，$K$，$V$表示，在self-attention里的把X映射到$QKV$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 $QKV$权重矩阵$W_Q$、$W_K$和$W_V$的维度探索\n",
    "![](https://img2020.cnblogs.com/blog/1470684/202103/1470684-20210309221742405-1927745420.png)\n",
    "* 现有一$3*4$维矩阵$X$,即矩阵包含3个字，其word embedding的大小为5，想要重新编码为字向量长度为10的矩阵\n",
    "* 即$Softmax(QK^T/\\sqrt d_k)V$的输出维度是$3*10$\n",
    "  * 思路：首先$Softmax(QK^T)$肯定是一个$3*3$的矩阵，因为它表示了每个字之间相互的注意力（相关程度）\n",
    "  * 若要求得输出结果为$3*10$的矩阵，那么$V$的维度就一定是$3*10$\n",
    "  * 而$V=XW_V$，已知$X$的维度是$3*5$，那么$W_V$的维度一定是$5*10$\n",
    "* 而对于$W_Q$和$W_K$的维度，因为要保证输出的$QK^T$的维度一定是$3*3$\n",
    "  * 则$Q$和$K$的维度保持一致并且一定是$3*N$\n",
    "  * 以$Q$为例:\n",
    "    * $Q=XW_Q$，已知$X$的维度是$3*5$，则$W_Q$的维度一定是$5*N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 1., 2., 1.],\n",
       "        [1., 1., 3., 2., 1.],\n",
       "        [3., 1., 2., 1., 1.]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size = 10\n",
    "embedding_size = 5\n",
    "dim_q = dim_k = 8\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 初始化各个权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机初始化WQ\n",
    "wq = torch.randn((embedding_size, dim_q), device=cuda0, dtype=torch.float64)\n",
    "# 随机初始化WK\n",
    "wk = torch.randn((embedding_size, dim_k), device=cuda0, dtype=torch.float64)\n",
    "# 随机初始化WV\n",
    "wv = torch.randn((embedding_size, output_size), device=cuda0, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 求出$Q$矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8332, -2.5610,  1.8061,  7.1666, -4.2463, -0.8057,  1.1351, -2.7823],\n",
       "        [-1.7030, -1.5517,  0.3341,  2.5253, -4.6023,  1.4423,  1.0379, -4.8009],\n",
       "        [-1.4010,  0.5444, -0.4832,  6.3650, -3.9602,  3.2184,  0.7200, -0.0427]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = X@wq\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 求出$K$矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2336,  3.1860,  2.0341,  3.5575, -0.8685, -3.7346,  3.8811,  1.3885],\n",
       "        [ 5.9752,  0.4658,  1.1580,  3.7000,  1.5736, -4.0247,  1.1894,  1.6964],\n",
       "        [ 2.9978,  3.7932,  2.9824, -0.4912, -3.6555, -0.5675, -0.5390,  5.0644]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = X@wk\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 求出$V$矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8362,  3.5635,  2.8100,  3.8693,  1.5980, -1.7478, -0.1589, -3.2644,\n",
       "         -0.9916,  1.6436],\n",
       "        [ 9.8297,  3.8295,  2.4308,  7.6969, -0.4390, -0.2967,  0.5100, -5.1272,\n",
       "         -3.8701,  0.7872],\n",
       "        [ 5.0569,  3.7814,  5.3357,  3.2810,  3.8979,  1.1134,  0.3318, -0.6731,\n",
       "         -3.3002, -1.3283]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = X@wv\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 求出$QK^T$矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15.2200, -14.2488, -24.0578],\n",
       "        [ -3.1114, -21.1249, -20.1033],\n",
       "        [ 14.4212,  -3.5277,   5.3432]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score = Q@K.T\n",
    "attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 $QK^T/\\sqrt{d_k}$：利用$d_k$对$QK^T$进行解耦，防止$QK^T$的变化过大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.3811, -5.0377, -8.5057],\n",
       "        [-1.1000, -7.4688, -7.1076],\n",
       "        [ 5.0987, -1.2472,  1.8891]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_attention_score = attention_score/sqrt(dim_k)\n",
    "scaled_attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 利用Softmax()对$QK^T/\\sqrt{d_k}$进行归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9997e-01, 2.9865e-05, 9.3117e-07],\n",
       "        [9.9584e-01, 1.7072e-03, 2.4499e-03],\n",
       "        [9.5958e-01, 1.6830e-03, 3.8742e-02]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_scaled_att_score = softmax(scaled_attention_score)\n",
    "soft_scaled_att_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.8 将注意力矩阵$Softmax(QK^T/\\sqrt{d_k})$乘上$V$，对原句进行表达"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8363,  3.5635,  2.8100,  3.8695,  1.5980, -1.7477, -0.1589, -3.2644,\n",
       "         -0.9917,  1.6436],\n",
       "        [ 7.8328,  3.5645,  2.8156,  3.8744,  1.6002, -1.7383, -0.1565, -3.2612,\n",
       "         -1.0022,  1.6349],\n",
       "        [ 7.7319,  3.5724,  2.9072,  3.8530,  1.6837, -1.6345, -0.1387, -3.1671,\n",
       "         -1.0859,  1.5270]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output = soft_scaled_att_score@V\n",
    "attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 当batch_size>1时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 定义一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 1., 2., 1.],\n",
       "          [1., 1., 3., 2., 1.],\n",
       "          [3., 1., 2., 1., 1.]],\n",
       " \n",
       "         [[3., 1., 2., 1., 1.],\n",
       "          [1., 1., 3., 2., 1.],\n",
       "          [1., 2., 1., 2., 1.]]], device='cuda:0', dtype=torch.float64),\n",
       " torch.Size([2, 3, 5]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = torch.tensor([[[1,2,1,2,1], # 早\n",
    "                    [1,1,3,2,1], # 上\n",
    "                    [3,1,2,1,1]], # 好\n",
    "                    \n",
    "                    [[3,1,2,1,1], # 好\n",
    "                    [1,1,3,2,1], # 早\n",
    "                    [1,2,1,2,1]] # 上\n",
    "                ], dtype=torch.float64, device=cuda0)\n",
    "X2, X2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = X2.size()[0]\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 先实现$Softmax(XX^T)X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 11., 10.],\n",
       "         [11., 16., 13.],\n",
       "         [10., 13., 16.]],\n",
       "\n",
       "        [[16., 13., 10.],\n",
       "         [13., 16., 11.],\n",
       "         [10., 11., 11.]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_dot = torch.bmm(X2, X2.permute(0, 2, 1))\n",
    "X2_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4223, 0.0064, 0.0024],\n",
       "         [0.4223, 0.9465, 0.0473],\n",
       "         [0.1554, 0.0471, 0.9503]],\n",
       "\n",
       "        [[0.9503, 0.0471, 0.1554],\n",
       "         [0.0473, 0.9465, 0.4223],\n",
       "         [0.0024, 0.0064, 0.4223]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_X2_dot = softmax(X2_dot)\n",
    "soft_X2_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4358, 0.8534, 0.4462, 0.8597, 0.4311],\n",
       "         [1.5108, 1.8385, 3.3564, 2.7849, 1.4161],\n",
       "         [3.0535, 1.3082, 2.1974, 1.3553, 1.1528]],\n",
       "\n",
       "        [[3.0535, 1.3082, 2.1974, 1.3553, 1.1528],\n",
       "         [1.5108, 1.8385, 3.3564, 2.7849, 1.4161],\n",
       "         [0.4358, 0.8534, 0.4462, 0.8597, 0.4311]]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(soft_X2_dot, X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 实现$Softmax(QK^T/\\sqrt{d_k})V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "dim_q = dim_k = 8\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3.1 定义qkv权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = nn.Linear(embedding_size, dim_q)\n",
    "# k = nn.Linear(embedding_size, dim_k)\n",
    "# v = nn.Linear(embedding_size, output_size)\n",
    "wq = torch.randn(batch_size, embedding_size, dim_q, device=cuda0, dtype=torch.float64)\n",
    "wk = torch.randn(batch_size, embedding_size, dim_k, device=cuda0, dtype=torch.float64)\n",
    "wv = torch.randn(batch_size, embedding_size, output_size, device=cuda0, dtype=torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5582, -0.8717, -1.3706, -1.0544,  0.4997,  1.3938, -3.4247,\n",
       "           5.0492],\n",
       "         [ 1.2169, -3.1864, -2.7058, -1.2611,  1.9660,  3.3174, -3.5892,\n",
       "           7.2948],\n",
       "         [ 3.3495, -2.1440, -6.9993, -7.4153, -1.1284,  4.5366, -7.4462,\n",
       "           8.1977]],\n",
       "\n",
       "        [[-2.5640, -4.1961,  5.0915, -4.3541,  2.0638,  0.5317, -3.3642,\n",
       "          -1.2488],\n",
       "         [-4.0804, -4.5291,  4.7965, -0.4264,  5.1302, -0.7123, -1.3305,\n",
       "          -1.7631],\n",
       "         [-0.5793, -5.0655,  3.4297, -3.4159,  3.6012, -1.2186,  0.1353,\n",
       "          -2.5812]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.bmm(X2, wq)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4164, -2.9752,  7.1337,  1.8657, -3.0496, -0.0352,  1.0243,\n",
       "           0.7829],\n",
       "         [-5.7465, -4.5874,  8.2959,  5.6329, -1.0198,  0.8279,  0.2871,\n",
       "           0.2422],\n",
       "         [-4.1353, -4.7825,  8.3578,  4.5925, -3.8728,  5.0690, -0.2854,\n",
       "           1.1925]],\n",
       "\n",
       "        [[ 2.4248, 11.3674, -2.6641,  3.5242,  7.8670,  5.0724, -5.4677,\n",
       "           0.1940],\n",
       "         [ 0.0773,  8.6462, -3.3254, -1.5883,  3.3939,  4.4412, -7.7996,\n",
       "          -1.4249],\n",
       "         [ 2.6913,  5.3212, -2.8643, -1.2792,  5.0154, -1.4874, -4.7116,\n",
       "          -2.5206]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.bmm(X2, wk)\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.8002, -0.6814, -1.6538, -3.0207, -2.9105, -2.0037, -5.2438,\n",
       "          -3.4670, -1.9490, -1.4147],\n",
       "         [-3.0350, -1.4567, -0.8261, -5.5171, -4.2596, -1.5180, -3.4168,\n",
       "          -4.7778,  1.9638, -1.9664],\n",
       "         [-6.9332,  0.0449, -1.4575, -6.8872, -4.6898,  2.3824, -0.2478,\n",
       "          -1.3238, -3.3458, -6.0011]],\n",
       "\n",
       "        [[-6.0053,  3.0739, -3.5484,  3.1885, -0.3823,  2.5164,  4.9871,\n",
       "          -4.8868, -5.7102, -2.2606],\n",
       "         [-5.7088,  2.0548, -3.5776, -2.1744,  2.3161,  0.2995,  1.5970,\n",
       "          -2.1827, -4.8814, -4.2858],\n",
       "         [-6.9242,  1.6778, -3.4086,  0.9004, -0.4242,  1.3941,  4.1422,\n",
       "          -2.0995, -7.1041,  0.1113]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.bmm(X2, wv)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -12.4868,  -21.3811,   -6.4442],\n",
       "         [ -17.9769,  -20.4481,    0.7261],\n",
       "         [ -60.0598, -104.4932,  -56.8849]],\n",
       "\n",
       "        [[ -45.7394,   -9.1095,   -9.6846],\n",
       "         [ -31.9809,  -27.6112,  -10.7733],\n",
       "         [ -59.2530,  -40.3890,   -8.2256]]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK_T = torch.bmm(Q, K.permute(0, 2, 1))\n",
    "QK_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -4.4147,  -7.5594,  -2.2784],\n",
       "         [ -6.3558,  -7.2295,   0.2567],\n",
       "         [-21.2343, -36.9439, -20.1119]],\n",
       "\n",
       "        [[-16.1713,  -3.2207,  -3.4240],\n",
       "         [-11.3069,  -9.7620,  -3.8089],\n",
       "         [-20.9491, -14.2797,  -2.9082]]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_QK_T = QK_T/sqrt(dim_k)\n",
    "scaled_QK_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[8.7447e-01, 4.1828e-01, 7.3434e-02],\n",
       "         [1.2553e-01, 5.8172e-01, 9.2657e-01],\n",
       "         [4.3360e-08, 7.2428e-14, 1.3210e-09]],\n",
       "\n",
       "        [[7.6571e-03, 9.9854e-01, 2.9801e-01],\n",
       "         [9.9228e-01, 1.4404e-03, 2.0280e-01],\n",
       "         [6.4431e-05, 1.5722e-05, 4.9919e-01]]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_scaled_QK_T = softmax(scaled_QK_T)\n",
    "soft_scaled_QK_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.9762e+00, -1.2019e+00, -1.8987e+00, -5.4549e+00, -4.6712e+00,\n",
       "          -2.2122e+00, -6.0329e+00, -5.1274e+00, -1.1286e+00, -2.5003e+00],\n",
       "         [-8.7922e+00, -8.9135e-01, -2.0387e+00, -9.9701e+00, -7.1887e+00,\n",
       "           1.0728e+00, -2.8755e+00, -4.4411e+00, -2.2023e+00, -6.8819e+00],\n",
       "         [-2.1729e-07, -2.9486e-08, -7.3633e-08, -1.4007e-07, -1.3240e-07,\n",
       "          -8.3735e-08, -2.2770e-07, -1.5208e-07, -8.8928e-08, -6.9270e-08]],\n",
       "\n",
       "        [[-7.8100e+00,  2.5753e+00, -4.6153e+00, -1.8785e+00,  2.1834e+00,\n",
       "           7.3379e-01,  2.8673e+00, -2.8426e+00, -7.0351e+00, -4.2637e+00],\n",
       "         [-7.3714e+00,  3.3933e+00, -4.2175e+00,  3.3433e+00, -4.6205e-01,\n",
       "           2.7801e+00,  5.7910e+00, -5.2780e+00, -7.1139e+00, -2.2268e+00],\n",
       "         [-3.4570e+00,  8.3776e-01, -1.7018e+00,  4.4964e-01, -2.1173e-01,\n",
       "           6.9607e-01,  2.0681e+00, -1.0484e+00, -3.5467e+00,  5.5369e-02]]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_attention_output = torch.bmm(soft_scaled_QK_T, V)\n",
    "batch_attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 集成self-attention类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 单头self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self, embedding_size, dim_k, output_size):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        # input (batch_size, seq_len, embedding_size)\n",
    "        # q (batch_size, embedding_size, dim_k)\n",
    "        # k (batch_size, embedding_size, dim_k)\n",
    "        # v (batch_size, embedding_size, output_size)\n",
    "        # self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dim_q = self.dim_k = dim_k\n",
    "        self.dim_v = output_size\n",
    "        self.output_size = output_size\n",
    "        self.norm_factor = 1/sqrt(self.dim_k)\n",
    "\n",
    "        self.q = nn.Linear(self.embedding_size, self.dim_q)\n",
    "        self.k = nn.Linear(self.embedding_size, self.dim_k)\n",
    "        self.v = nn.Linear(self.embedding_size, self.dim_v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.q(x) # dim: batch_size * seq_len * dim_q\n",
    "        K = self.k(x) # dim: batch_size * seq_len * dim_k\n",
    "        V = self.v(x) # dim: batch_size * seq_len * output_size(dim_v)\n",
    "\n",
    "        attention_score = torch.bmm(Q, K.permute(0, 2, 1)) * self.norm_factor # dim: batch_size * seq_len * seq_len\n",
    "        soft_attention_score = nn.Softmax(dim=-1)(attention_score) # dim: batch_size * seq_len * seq_len\n",
    "        output = torch.bmm(soft_attention_score, V) # dim: batch_size * seq_len * output_size(dim_v)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2042,  0.0748,  1.2078, -0.9053],\n",
       "          [ 0.0406,  1.0886, -1.9610,  0.4739],\n",
       "          [-0.1113,  1.6448,  1.4125,  0.5346]],\n",
       " \n",
       "         [[ 0.6377,  0.0266, -0.5305,  1.2798],\n",
       "          [ 0.0883,  0.0724,  0.2831, -0.3107],\n",
       "          [-0.2889,  0.6488,  0.2794,  0.3465]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "embedding_size = 4\n",
    "x = torch.randn(batch_size, seq_len, embedding_size) # dim: batch_size=2, seq_len=3, embedding_size=4\n",
    "x, x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3356, -0.1771,  0.3725,  0.6132, -0.2158],\n",
       "          [ 0.3355, -0.1658,  0.3703,  0.5375, -0.2064],\n",
       "          [ 0.3772, -0.2019,  0.4047,  0.7072, -0.1733]],\n",
       " \n",
       "         [[ 0.2221, -0.4991,  0.2550,  0.4026, -0.4349],\n",
       "          [ 0.2605, -0.4543,  0.2559,  0.4056, -0.3882],\n",
       "          [ 0.2621, -0.4526,  0.2555,  0.4072, -0.3866]]],\n",
       "        grad_fn=<BmmBackward0>),\n",
       " torch.Size([2, 3, 5]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention = Self_Attention(embedding_size=embedding_size, dim_k=10, output_size=5)\n",
    "res = self_attention(x)\n",
    "res, res.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 多头self-attention\n",
    "![](https://pic2.zhimg.com/80/v2-ed923dba9f3c3f32decd2b8a6283da41_720w.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_nums = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 2., 1., 2., 1.],\n",
       "          [1., 1., 3., 2., 1.],\n",
       "          [3., 1., 2., 1., 1.]],\n",
       " \n",
       "         [[3., 1., 2., 1., 1.],\n",
       "          [1., 1., 3., 2., 1.],\n",
       "          [1., 2., 1., 2., 1.]]], device='cuda:0', dtype=torch.float64),\n",
       " torch.Size([2, 3, 5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2, X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.5582, -0.8717, -1.3706, -1.0544],\n",
       "           [ 0.4997,  1.3938, -3.4247,  5.0492],\n",
       "           [ 1.2169, -3.1864, -2.7058, -1.2611]],\n",
       " \n",
       "          [[ 1.9660,  3.3174, -3.5892,  7.2948],\n",
       "           [ 3.3495, -2.1440, -6.9993, -7.4153],\n",
       "           [-1.1284,  4.5366, -7.4462,  8.1977]]],\n",
       " \n",
       " \n",
       "         [[[-2.5640, -4.1961,  5.0915, -4.3541],\n",
       "           [ 2.0638,  0.5317, -3.3642, -1.2488],\n",
       "           [-4.0804, -4.5291,  4.7965, -0.4264]],\n",
       " \n",
       "          [[ 5.1302, -0.7123, -1.3305, -1.7631],\n",
       "           [-0.5793, -5.0655,  3.4297, -3.4159],\n",
       "           [ 3.6012, -1.2186,  0.1353, -2.5812]]]], device='cuda:0',\n",
       "        dtype=torch.float64),\n",
       " torch.Size([2, 2, 3, 4]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_mh = torch.bmm(X2, wq).reshape(-1, # batch_size\n",
    "                                 X2.shape[0], # seq_len\n",
    "                                 X2.shape[1], # embedding_size\n",
    "                                 dim_q//head_nums # segmented by head_nums\n",
    "                                )\n",
    "Q_mh, Q_mh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.4164, -2.9752,  7.1337,  1.8657],\n",
       "           [-3.0496, -0.0352,  1.0243,  0.7829],\n",
       "           [-5.7465, -4.5874,  8.2959,  5.6329]],\n",
       " \n",
       "          [[-1.0198,  0.8279,  0.2871,  0.2422],\n",
       "           [-4.1353, -4.7825,  8.3578,  4.5925],\n",
       "           [-3.8728,  5.0690, -0.2854,  1.1925]]],\n",
       " \n",
       " \n",
       "         [[[ 2.4248, 11.3674, -2.6641,  3.5242],\n",
       "           [ 7.8670,  5.0724, -5.4677,  0.1940],\n",
       "           [ 0.0773,  8.6462, -3.3254, -1.5883]],\n",
       " \n",
       "          [[ 3.3939,  4.4412, -7.7996, -1.4249],\n",
       "           [ 2.6913,  5.3212, -2.8643, -1.2792],\n",
       "           [ 5.0154, -1.4874, -4.7116, -2.5206]]]], device='cuda:0',\n",
       "        dtype=torch.float64),\n",
       " torch.Size([2, 2, 3, 4]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_mh = torch.bmm(X2, wk).reshape(-1, X2.shape[0], X2.shape[1], dim_k//head_nums)\n",
    "K_mh, K_mh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-4.8002, -0.6814, -1.6538, -3.0207, -2.9105],\n",
       "           [-2.0037, -5.2438, -3.4670, -1.9490, -1.4147],\n",
       "           [-3.0350, -1.4567, -0.8261, -5.5171, -4.2596]],\n",
       " \n",
       "          [[-1.5180, -3.4168, -4.7778,  1.9638, -1.9664],\n",
       "           [-6.9332,  0.0449, -1.4575, -6.8872, -4.6898],\n",
       "           [ 2.3824, -0.2478, -1.3238, -3.3458, -6.0011]]],\n",
       " \n",
       " \n",
       "         [[[-6.0053,  3.0739, -3.5484,  3.1885, -0.3823],\n",
       "           [ 2.5164,  4.9871, -4.8868, -5.7102, -2.2606],\n",
       "           [-5.7088,  2.0548, -3.5776, -2.1744,  2.3161]],\n",
       " \n",
       "          [[ 0.2995,  1.5970, -2.1827, -4.8814, -4.2858],\n",
       "           [-6.9242,  1.6778, -3.4086,  0.9004, -0.4242],\n",
       "           [ 1.3941,  4.1422, -2.0995, -7.1041,  0.1113]]]], device='cuda:0',\n",
       "        dtype=torch.float64),\n",
       " torch.Size([2, 2, 3, 5]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_mh = torch.bmm(X2, wv).reshape(-1, X2.shape[0], X2.shape[1], output_size//head_nums)\n",
    "V_mh, V_mh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1.7325e-01, 8.2309e-01, 3.6642e-03],\n",
       "           [1.2543e-03, 9.4497e-01, 5.3775e-02],\n",
       "           [8.9611e-02, 9.0514e-01, 5.2456e-03]],\n",
       " \n",
       "          [[2.0897e-03, 8.8452e-07, 9.9791e-01],\n",
       "           [9.9953e-01, 4.1441e-14, 4.6726e-04],\n",
       "           [5.0206e-06, 3.8078e-13, 9.9999e-01]]],\n",
       " \n",
       " \n",
       "         [[[2.6388e-06, 2.3406e-04, 9.9976e-01],\n",
       "           [5.0330e-04, 9.9835e-01, 1.1421e-03],\n",
       "           [6.1493e-04, 8.1297e-05, 9.9930e-01]],\n",
       " \n",
       "          [[2.4960e-02, 5.0111e-04, 9.7454e-01],\n",
       "           [2.1488e-07, 1.7090e-05, 9.9998e-01],\n",
       "           [3.1182e-03, 9.6729e-04, 9.9591e-01]]]], device='cuda:0',\n",
       "        dtype=torch.float64),\n",
       " torch.Size([2, 2, 3, 3]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q_mh · K_mh.T\n",
    "atten_score_mh = nn.Softmax(dim=-1)(Q_mh@K_mh.permute(0, 1, 3, 2)/sqrt(dim_k))\n",
    "atten_score_mh, atten_score_mh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-2.4920e+00, -4.4395e+00, -3.1432e+00, -2.1477e+00, -1.6843e+00],\n",
       "           [-2.0627e+00, -5.0345e+00, -3.3227e+00, -2.1422e+00, -1.5696e+00],\n",
       "           [-2.2597e+00, -4.8151e+00, -3.2906e+00, -2.0637e+00, -1.5637e+00]],\n",
       " \n",
       "          [[ 2.3742e+00, -2.5439e-01, -1.3310e+00, -3.3347e+00, -5.9927e+00],\n",
       "           [-1.5162e+00, -3.4153e+00, -4.7762e+00,  1.9613e+00, -1.9683e+00],\n",
       "           [ 2.3823e+00, -2.4779e-01, -1.3238e+00, -3.3457e+00, -6.0011e+00]]],\n",
       " \n",
       " \n",
       "         [[[-5.7069e+00,  2.0555e+00, -3.5779e+00, -2.1752e+00,  2.3151e+00],\n",
       "           [ 2.5027e+00,  4.9828e+00, -4.8846e+00, -5.7017e+00, -2.2545e+00],\n",
       "           [-5.7083e+00,  2.0557e+00, -3.5777e+00, -2.1714e+00,  2.3141e+00]],\n",
       " \n",
       "          [[ 1.3626e+00,  4.0775e+00, -2.1022e+00, -7.0446e+00,  1.3222e-03],\n",
       "           [ 1.3939e+00,  4.1422e+00, -2.0995e+00, -7.1040e+00,  1.1133e-01],\n",
       "           [ 1.3826e+00,  4.1319e+00, -2.1010e+00, -7.0894e+00,  9.7115e-02]]]],\n",
       "        device='cuda:0', dtype=torch.float64),\n",
       " torch.Size([2, 2, 3, 5]))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_score_mhV_mh = (atten_score_mh@V_mh)\n",
    "atten_score_mhV_mh, atten_score_mhV_mh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.4920e+00, -4.4395e+00, -3.1432e+00, -2.1477e+00, -1.6843e+00,\n",
       "           -2.0627e+00, -5.0345e+00, -3.3227e+00, -2.1422e+00, -1.5696e+00],\n",
       "          [-2.2597e+00, -4.8151e+00, -3.2906e+00, -2.0637e+00, -1.5637e+00,\n",
       "            2.3742e+00, -2.5439e-01, -1.3310e+00, -3.3347e+00, -5.9927e+00],\n",
       "          [-1.5162e+00, -3.4153e+00, -4.7762e+00,  1.9613e+00, -1.9683e+00,\n",
       "            2.3823e+00, -2.4779e-01, -1.3238e+00, -3.3457e+00, -6.0011e+00]],\n",
       " \n",
       "         [[-5.7069e+00,  2.0555e+00, -3.5779e+00, -2.1752e+00,  2.3151e+00,\n",
       "            2.5027e+00,  4.9828e+00, -4.8846e+00, -5.7017e+00, -2.2545e+00],\n",
       "          [-5.7083e+00,  2.0557e+00, -3.5777e+00, -2.1714e+00,  2.3141e+00,\n",
       "            1.3626e+00,  4.0775e+00, -2.1022e+00, -7.0446e+00,  1.3222e-03],\n",
       "          [ 1.3939e+00,  4.1422e+00, -2.0995e+00, -7.1040e+00,  1.1133e-01,\n",
       "            1.3826e+00,  4.1319e+00, -2.1010e+00, -7.0894e+00,  9.7115e-02]]],\n",
       "        device='cuda:0', dtype=torch.float64),\n",
       " torch.Size([2, 3, 10]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat multi-head\n",
    "res_mh = atten_score_mhV_mh.reshape(X2.shape[0], X2.shape[1], -1)\n",
    "res_mh, res_mh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention_multi_head(nn.Module):\n",
    "    # input: batch_size, seq_len, embedding_size\n",
    "    # q: batch_size, embedding_size, dim_q\n",
    "    # k: batch_size, embedding_size, dim_k\n",
    "    # v: batch_size, embedding_size, dim_v\n",
    "    def __init__(self, embedding_size, dim_k, output_size, head_nums):\n",
    "        super(Self_Attention_multi_head, self).__init__()\n",
    "        assert dim_k % head_nums == 0\n",
    "        assert output_size % head_nums == 0\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dim_q = self.dim_k = dim_k\n",
    "        self.dim_v = output_size\n",
    "        self.q = nn.Linear(self.embedding_size, self.dim_q)\n",
    "        self.k = nn.Linear(self.embedding_size, self.dim_k)\n",
    "        self.v = nn.Linear(self.embedding_size, self.dim_v)\n",
    "        self.norm_factor = 1/sqrt(dim_k)\n",
    "        self.head_nums = head_nums\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.q(x).reshape(-1, x.shape[0], x.shape[1], self.dim_q//self.head_nums) \n",
    "        # Q dim: batch_size, multi-segment(head_nums), seq_len, embedding_size/head_nums\n",
    "        K = self.k(x).reshape(-1, x.shape[0], x.shape[1], self.dim_k//self.head_nums)\n",
    "        # K dim: batch_size, multi-segment(head_nums), seq_len, embedding_size/head_nums\n",
    "        V = self.v(x).reshape(-1, x.shape[0], x.shape[1], self.dim_v//self.head_nums)\n",
    "        # V dim: batch_size, multi-segment(head_nums), seq_len, outputs_size/head_nums\n",
    "\n",
    "        QK = Q @ K.permute(0, 1, 3, 2)\n",
    "        # QK dim: batch_size, multi-segment(head_nums), seq_len, seq_len\n",
    "        scaled_QK = QK * self.norm_factor\n",
    "        atten_mh = nn.Softmax(dim=-1)(scaled_QK)\n",
    "        # atten_mh dim: batch_size, multi-segment(head_nums), seq_len, seq_len\n",
    "        atten_mhV = atten_mh @ V\n",
    "        # atten_mhV dim: batch_size, multi-segment(head_nums), seq_len, output_size/head_nums\n",
    "\n",
    "        output = atten_mhV.reshape(x.shape[0], x.shape[1], -1)\n",
    "        # concat multi-segment\n",
    "        # output dim: batch_size, seq_len, output_size\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.9022, -0.9151,  0.2304,  0.5386],\n",
       "          [ 1.2929, -0.2612,  1.4492, -1.6717],\n",
       "          [ 0.9719,  0.4376,  0.2973, -0.4038]],\n",
       " \n",
       "         [[-1.6375,  0.1130, -1.7010,  1.8347],\n",
       "          [ 0.8031, -1.2434,  0.1808,  1.2782],\n",
       "          [-0.2180, -1.1914,  0.6073, -1.2531]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "embedding_size = 4\n",
    "x = torch.randn(batch_size, seq_len, embedding_size) # dim: batch_size=2, seq_len=3, embedding_size=4\n",
    "x, x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1972, -0.3130,  0.0528,  0.0773,  0.3722,  0.2201, -0.2592,\n",
       "            0.0494,  0.0707,  0.3857],\n",
       "          [ 0.2272, -0.4015, -0.0439,  0.1180,  0.3787,  0.1338, -0.5360,\n",
       "            0.0356,  0.1468,  0.2021],\n",
       "          [ 0.1159, -0.5426,  0.0500,  0.1513,  0.2313,  0.0940, -0.5458,\n",
       "            0.0652,  0.1568,  0.2582]],\n",
       " \n",
       "         [[ 0.0424,  1.4424,  0.5992, -0.5465,  0.3334,  0.0136,  1.3545,\n",
       "            0.6691, -0.5385,  0.3220],\n",
       "          [ 0.0192,  0.7246,  0.4727, -0.5457, -0.4344,  0.5148, -0.1021,\n",
       "           -0.2016, -0.1437,  0.4408],\n",
       "          [ 0.5322, -0.0837, -0.2489, -0.0924,  0.4787,  0.5260, -0.0898,\n",
       "           -0.2341, -0.1079,  0.4670]]], grad_fn=<ViewBackward>),\n",
       " torch.Size([2, 3, 10]))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_k = 6\n",
    "output_size = 10\n",
    "head_nums = 2\n",
    "self_attention_mh = Self_Attention_multi_head(embedding_size=embedding_size, \n",
    "                                              dim_k=dim_k, \n",
    "                                              output_size=output_size,\n",
    "                                              head_nums=head_nums)\n",
    "res = self_attention_mh(x)\n",
    "res, res.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. head_nums的确定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "embedding_size = 100\n",
    "x2 = torch.randn(batch_size, seq_len, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_nums = 4\n",
    "dim_k = 64\n",
    "output_size = 100\n",
    "self_attention_mh = Self_Attention_multi_head(embedding_size=embedding_size,\n",
    "                                              dim_k=dim_k,\n",
    "                                              output_size=output_size,\n",
    "                                              head_nums=head_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = self_attention_mh(x2)\n",
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d5334fe9b82e8c016b8b2657765205a9a9ba0b9bc469dbf5ca7c631ff3e3ab7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

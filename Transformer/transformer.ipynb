{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [熬了一晚上，我从零实现了Transformer模型，把代码讲给你听](https://mp.weixin.qq.com/s/q30K-W4v853rEcDsP4SgyA)\n",
    "* ![](https://miro.medium.com/max/958/1*2KrICIr3FUjUj1ukBvbNKw.png)![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfrc46CPgjmcu492RZKlbMIeKcBYEayBkpjfx2gBAe81a1WicMMBLeicjH2KR46KzkUgOgZd37K9ZXfQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available')\n",
    "    cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, \n",
    "                 vocab_size, d_model, n_heads, padding_size, N, dropout, mask=True \n",
    "                ):\n",
    "        super(Config, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model # embedding size\n",
    "        assert self.d_model % n_heads == 0\n",
    "        self.n_heads = n_heads # number of heads\n",
    "        self.padding_size = padding_size # or seq_size\n",
    "        self.N = N # number of encoder and decoder\n",
    "        self.PAD = 0 # padding index\n",
    "        self.UNK = 1 # unkown index\n",
    "        self.dropout = dropout\n",
    "        self.mask = mask # whether use masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(vocab_size=6, d_model=20, n_heads=5, padding_size=8, N=10, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "* input: [batch_size * seq_len]\n",
    "* output: [batch_size * seq_len * d_model]\n",
    "* add a normal embedding layer and position embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, padding_size, PAD, UNK, vocab_size, d_model, seq_len):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.padding_szie = padding_size\n",
    "        self.PAD = PAD\n",
    "        self.UNK = UNK\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding = nn.Embedding(\n",
    "            embedding_dim=self.d_model, \n",
    "            padding_idx=self.PAD, \n",
    "            vocab_size=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size_ = x.shape[0]\n",
    "        for i in range(batch_size_):\n",
    "            if len(x[i]) < self.seq_len:\n",
    "                x[i].extend([self.PAD] * (self.padding_szie - len(x[i])))\n",
    "            else:\n",
    "                x[i] = x[:self.padding_szie]\n",
    "        x = self.embedding(torch.tensor(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embedding\n",
    "![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfrc46CPgjmcu492RZKlbMIe0MYRn97p9ffMJQhuOGqPmQteX8JmS68FMbZ0aW1WicKLRWmzniaxC1Ng/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, seq_len):\n",
    "        position_embed = np.zeros((seq_len, self.d_model))\n",
    "        for pos in range(position_embed.shape[0]):\n",
    "            for i in range(position_embed.shape[1]):\n",
    "                position_embed[pos][i] = math.sin(pos/(10000**((2*i)/self.d_model))) if i % 2 == 0 else math.cos(pos/(10000**((2*i)/self.d_model)))\n",
    "        return torch.from_numpy(position_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Self-attention\n",
    "![](https://camo.githubusercontent.com/9ebfaedcab26d98abf2520743e52a3eefb56811036d843811d38b55c5d8668c1/68747470733a2f2f706963322e7a68696d672e636f6d2f38302f76322d65643932336462613966336333663332646563643262386136323833646134315f373230772e6a7067)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, n_heads, dim_k, dim_v):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self.n_heads = n_heads\n",
    "        assert self.dim_k % self.n_heads == 0 and self.dim_v % self.n_heads == 0 # dim_v also is output size\n",
    "\n",
    "        self.q = nn.Linear(self.d_model, self.dim_k, bias=True)\n",
    "        self.k = nn.Linear(self.d_model, self.dim_k, bias=True)\n",
    "        self.v = nn.Linear(self.d_model, self.dim_v, bias=True)\n",
    "        self.norm_factor = 1 / math.sqrt(dim_k)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.o = nn.Linear(self.dim_v, self.d_model)\n",
    "\n",
    "    def generate_mask(self, dim):\n",
    "        # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "        init_mt = np.ones((dim, dim))\n",
    "        masking = torch.tensor(np.tril(init_mt))\n",
    "        return masking == 1\n",
    "\n",
    "    def forward(self, x, y, requires_mask=False):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        Q = self.q(x).reshape(-1, x.shape[1], self.n_heads, self.dim_k//self.n_heads)\n",
    "        K = self.k(x).reshape(-1, x.shape[1], self.n_heads, self.dim_k//self.n_heads)\n",
    "        V = self.v(y).reshape(-1, y.shape[1], self.n_heads, self.dim_v//self.n_heads)\n",
    "        scaled_attention_score = (Q @ K.permute(0, 1, 3, 2)) * self.norm_factor\n",
    "        if requires_mask:\n",
    "            masking = self.generate_mask(x.shape[1])\n",
    "            scaled_attention_score = scaled_attention_score.fill_mask(masking, value=float(\"-inf\"))\n",
    "        output = (self.softmax(scaled_attention_score) @ V).reshape(y.shape[0], y.shape[1], -1)\n",
    "        output = self.o(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward\n",
    "![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfrc46CPgjmcu492RZKlbMIeXs8Btdy2AKetj5jeRKnLdRQKJU1nxwmrYazXuMTFtKGF63udn9oAdA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input, hidden_dim=2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.L1 = nn.Linear(input, hidden_dim, bias=True)\n",
    "        self.L2 = nn.Linear(hidden_dim, input, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.L2(nn.ReLU(self.L1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add & LayerNorm\n",
    "![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfrc46CPgjmcu492RZKlbMIemOsN9yqOQNdS8a4A6lQXDTfB2ey09cD5uUHexdiaSJYrdqnYwMsmSng/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayerNorm(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(AddLayerNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer, **kwarg):\n",
    "        # sublayer can be FFN or Multi-head self-attention layer\n",
    "        subout = sublayer(x, **kwarg)\n",
    "        x = self.dropout(x + subout)\n",
    "        layer_norm = nn.LayerNorm(x.size()[1:])\n",
    "        output = layer_norm(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "![](https://miro.medium.com/max/958/1*2KrICIr3FUjUj1ukBvbNKw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, seq_size, dim_k, dim_v, n_heads, mask, PAD, UNK, vocab_size, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_size = seq_size\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self.n_heads = n_heads\n",
    "        self.mask = mask\n",
    "        self.PAD = PAD\n",
    "        self.UNK = UNK\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.position_embedding = PositionalEmbedding(d_model=self.d_model)\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_model=self.d_model,\n",
    "            seq_len=self.seq_size,\n",
    "            n_heads=self.n_heads,\n",
    "            dim_k=self.dim_k,\n",
    "            dim_v=self.dim_v\n",
    "        )\n",
    "        self.ffn = FeedForward(input=self.d_model) # or self.seq_size? to be confirmed\n",
    "        self.add_norm = AddLayerNorm(dropout=self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x tensor: [batch_size, seq_size]\n",
    "        x += self.position_embedding(self.seq_size)\n",
    "        output = self.add_norm(x, self.attention, y=x)\n",
    "        output = self.add_norm(output, self.ffn)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, seq_size, dim_k, dim_v, n_heads, mask, PAD, UNK, vocab_size, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.embedding = Embedding(\n",
    "            padding_size=self.seq_size, PAD=self.PAD,\n",
    "            UNK=self.UNK,\n",
    "            vocab_size=self.vocab_size,\n",
    "            d_model=self.d_model,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 20])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, config.padding_size, config.d_model)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d5334fe9b82e8c016b8b2657765205a9a9ba0b9bc469dbf5ca7c631ff3e3ab7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
